{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGBLQiAkyz2O"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('en-fr.txt', names=['en', 'fr', 'attr'], usecols=['en', 'fr'], sep='\\t')\n",
        "df = df.sample(frac=1, random_state=42)\n",
        "df = df.reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "def clean_text(text):\n",
        "    text = normalize('NFD', text.lower())\n",
        "    text = re.sub('[^A-Za-z ]+', '', text)\n",
        "    return text\n",
        "\n",
        "def clean_and_prepare_text(text):\n",
        "    text = '[start] ' + clean_text(text) + ' [end]'\n",
        "    return text\n",
        "\n",
        "df['en'] = df['en'].apply(lambda row: clean_text(row))\n",
        "df['fr'] = df['fr'].apply(lambda row: clean_and_prepare_text(row))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LcNySlSUy5_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = df['en']\n",
        "fr = df['fr']\n",
        "\n",
        "en_max_len = max(len(line.split()) for line in en)\n",
        "fr_max_len = max(len(line.split()) for line in fr)\n",
        "sequence_len = max(en_max_len, fr_max_len)\n",
        "\n",
        "print(f'Max phrase length (English): {en_max_len}')\n",
        "print(f'Max phrase length (French): {fr_max_len}')\n",
        "print(f'Sequence length: {sequence_len}')"
      ],
      "metadata": {
        "id": "9HkQ4HZIzQkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "en_tokenizer = Tokenizer()\n",
        "en_tokenizer.fit_on_texts(en)\n",
        "en_sequences = en_tokenizer.texts_to_sequences(en)\n",
        "en_x = pad_sequences(en_sequences, maxlen=sequence_len, padding='post')\n",
        "\n",
        "fr_tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n')\n",
        "fr_tokenizer.fit_on_texts(fr)\n",
        "fr_sequences = fr_tokenizer.texts_to_sequences(fr)\n",
        "fr_y = pad_sequences(fr_sequences, maxlen=sequence_len + 1, padding='post')"
      ],
      "metadata": {
        "id": "3573oBWJzTc_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
        "fr_vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "\n",
        "print(f'Vocabulary size (English): {en_vocab_size}')\n",
        "print(f'Vocabulary size (French): {fr_vocab_size}')"
      ],
      "metadata": {
        "id": "EUycmVb_zVag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = { 'encoder_input': en_x, 'decoder_input': fr_y[:, :-1] }\n",
        "outputs = fr_y[:, 1:]"
      ],
      "metadata": {
        "id": "mHZl5jAPzYsy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-nlp"
      ],
      "metadata": {
        "id": "ZWoEWfsH0Pso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from keras_nlp.layers import TokenAndPositionEmbedding, TransformerEncoder\n",
        "from keras_nlp.layers import TransformerDecoder\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "num_heads = 8\n",
        "embed_dim = 256\n",
        "\n",
        "encoder_input = Input(shape=(None,), dtype='int64', name='encoder_input')\n",
        "x = TokenAndPositionEmbedding(en_vocab_size, sequence_len, embed_dim)(encoder_input)\n",
        "encoder_output = TransformerEncoder(embed_dim, num_heads)(x)\n",
        "encoded_seq_input = Input(shape=(None, embed_dim))\n",
        "\n",
        "decoder_input = Input(shape=(None,), dtype='int64', name='decoder_input')\n",
        "x = TokenAndPositionEmbedding(fr_vocab_size, sequence_len, embed_dim, mask_zero=True)(decoder_input)\n",
        "x = TransformerDecoder(embed_dim, num_heads)(x, encoded_seq_input)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "decoder_output = Dense(fr_vocab_size, activation='softmax')(x)\n",
        "decoder = Model([decoder_input, encoded_seq_input], decoder_output)\n",
        "decoder_output = decoder([decoder_input, encoder_output])\n",
        "\n",
        "model = Model([encoder_input, decoder_input], decoder_output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary(line_length=120)"
      ],
      "metadata": {
        "id": "XO8XiCD-zZWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "hist = model.fit(inputs, outputs, epochs=50, validation_split=0.2, callbacks=[callback])"
      ],
      "metadata": {
        "id": "nh8i_hH4zchw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set()\n",
        "\n",
        "acc = hist.history['accuracy']\n",
        "val = hist.history['val_accuracy']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
        "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "ofJDdlINzeYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text, model, en_tokenizer, fr_tokenizer, fr_index_lookup, sequence_len):\n",
        "    input_sequence = en_tokenizer.texts_to_sequences([text])\n",
        "    padded_input_sequence = pad_sequences(input_sequence, maxlen=sequence_len, padding='post')\n",
        "    decoded_text = '[start]'\n",
        "\n",
        "    for i in range(sequence_len):\n",
        "        target_sequence = fr_tokenizer.texts_to_sequences([decoded_text])\n",
        "        padded_target_sequence = pad_sequences(target_sequence, maxlen=sequence_len, padding='post')[:, :-1]\n",
        "\n",
        "        prediction = model([padded_input_sequence, padded_target_sequence])\n",
        "\n",
        "        idx = np.argmax(prediction[0, i, :]) - 1\n",
        "        token = fr_index_lookup[idx]\n",
        "        decoded_text += ' ' + token\n",
        "\n",
        "        if token == '[end]':\n",
        "            break\n",
        "\n",
        "    return decoded_text[8:-6] # Remove [start] and [end] tokens\n",
        "\n",
        "fr_vocab = fr_tokenizer.word_index\n",
        "fr_index_lookup = dict(zip(range(len(fr_vocab)), fr_vocab))\n",
        "texts = en[40000:40010].values\n",
        "\n",
        "for text in texts:\n",
        "    translated = translate_text(text, model, en_tokenizer, fr_tokenizer, fr_index_lookup, sequence_len)\n",
        "    print(f'{text} => {translated}')"
      ],
      "metadata": {
        "id": "r28JcL_-zivQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_text('Hello world', model, en_tokenizer, fr_tokenizer, fr_index_lookup, sequence_len)"
      ],
      "metadata": {
        "id": "rhfpIgGWzkzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gS0rn-r4z-FR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}